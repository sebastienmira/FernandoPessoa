{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../tokenizer/\"))\n",
    "from tok import RegexTokenizer\n",
    "\n",
    "\n",
    "with open('../tokenizer/toks_vocab_5k.pkl', 'rb') as f: #loading vocab from tokenizer\n",
    "    tok_dict = pickle.load(f)\n",
    "\n",
    "\n",
    "with open('../poems.txt', 'r', encoding='utf-8') as f:\n",
    "    poems = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating tokenizer and adding vocab and merges trained a priori\n",
    "vocab3k = dict(list(tok_dict['vocab'].items())[:3000]) #cutting the vocab size to 3k\n",
    "merges3k= dict(list(tok_dict['merges'].items())[:(3000-256)])\n",
    "\n",
    "tokenizer = RegexTokenizer()\n",
    "tokenizer.vocab = vocab3k\n",
    "tokenizer.merges = merges3k\n",
    "\n",
    "#encoding the dataset\n",
    "encoded_poems = tokenizer.encode(poems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining hyperparameters\n",
    "\n",
    "parameters = {'block_size':8, 'n_embd': 32, 'n_head':4, 'n_layer':4, 'dropout':0.2, 'vocab_size': len(tokenizer.vocab)}\n",
    "batch_size = 32\n",
    "block_size = parameters['block_size']\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with 1349346 now is represented by 423242 tokens\n"
     ]
    }
   ],
   "source": [
    "#making it a torch tensor and splitting into training and validation sets\n",
    "data = torch.tensor(encoded_poems, dtype=torch.long)\n",
    "print(f'Dataset with {len(poems)} now is represented by {len(data)} tokens')\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loading\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i: i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1: i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x,y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split)\n",
    "            logits, loss = model(X,Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import GPTLanguageModel\n",
    "\n",
    "model = GPTLanguageModel(parameters)\n",
    "m = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 8.1673, val loss 8.1714\n",
      "step 500: train loss 6.6401, val loss 6.7454\n",
      "step 1000: train loss 6.2561, val loss 6.3639\n",
      "step 1500: train loss 5.9364, val loss 6.1022\n",
      "step 2000: train loss 5.7689, val loss 5.9374\n",
      "step 2500: train loss 5.6297, val loss 5.7953\n",
      "step 3000: train loss 5.4983, val loss 5.7099\n",
      "step 3500: train loss 5.4087, val loss 5.5880\n",
      "step 4000: train loss 5.3391, val loss 5.5241\n",
      "step 4500: train loss 5.2656, val loss 5.4918\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    #sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000 frubeias! Irow, and sti monte, qucer on nows on, the confuso da relig which\n",
      "In no chão odiar em que não são se... Seg também, não partSoarmos ao seio.\n",
      "O seu ar que interessa trabalinação\n",
      "AT, o femta a sorte,\n",
      "\n",
      "Co também dos jardins irmã.\n",
      "\n",
      "«ire this-me umístplrom notara de gas come —\n",
      "Minha alma, grional\n",
      "Vós que não fiz Emgal?\n",
      "É todos os meusviate para os meus versos\n",
      "\n",
      "Dbo E levira, Há e consegue que tudo — olhando lá. O momento mais\n",
      "E se deseja saber a infrvaseiro doves\n",
      "Néória com um olhar de olhar. Uma mesma obra dormualidade que\n",
      "\n",
      "\n",
      "\n",
      "es. Que há o com um liga à obra os melizar,\n",
      "Da vida, o que eu conceispada tão!...,\n",
      "Que se é a água à realidade na estrada,\n",
      "E tudo a vontade de mim da falsa\n",
      "E ele tem tão realidade não no mundo e tocado ir\n",
      "(Se abpobgo não houvesse, nem bcos erTA deles,  Ah no vez que em que fui para me preito sorri\n",
      "Intereé essas tirar que esse aras de que ves. Um com grandes o rierro fativo noind, do cais\n",
      "\n",
      "Moré-me só isso? ainda melhor fisos.\n",
      "Cin] que me apavoravas assim não sabe, ao Ser.\n",
      "\n",
      "Sur passece sem grandejou o Mstca a bordo, Viras ess ButternRres sangue, Fes entre da porta, \n",
      "That here,\n",
      "Como a Apolo à lumam da corre dode ajo!\n",
      "Na noite só finto, nem a fio, tu rendos, \n",
      "        Thebante da bRANlento\n",
      "The glória: a pouco em\n",
      "        Da rabre do que ficar das árvores...\n",
      "\n",
      "\n",
      "Fernando Pessoa\n",
      "Ali, Como o pensamento (otheifio —\n",
      "TO pração a flauta do sol de que haja.\n",
      "Será o sopro de elas quem.\n",
      "Na ausência que se o poder recordar fosção, serplardos são para todas as ilhas o pensamento do universo da noite e aos  Sisttur váic ams shado yet po th nightw and the sha we my love that plicas\n",
      "Man and hall,\n",
      "A esses a tudo, new bela: eu eles juntos como vantas\n",
      "The l\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(tokenizer.decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
