{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tok import RegexTokenizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../poems2.txt', 'r') as f:\n",
    "    poems = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging (32, 32) into 256\n",
      "Merging (32, 97) into 257\n",
      "Merging (32, 100) into 258\n",
      "Merging (32, 101) into 259\n",
      "Merging (32, 115) into 260\n",
      "Merging (101, 114) into 261\n",
      "Merging (32, 109) into 262\n",
      "Merging (113, 117) into 263\n",
      "Merging (32, 99) into 264\n",
      "Merging (101, 110) into 265\n",
      "Merging (111, 115) into 266\n",
      "Merging (32, 116) into 267\n",
      "Merging (111, 114) into 268\n",
      "Merging (101, 115) into 269\n",
      "Merging (32, 112) into 270\n",
      "Merging (97, 115) into 271\n",
      "Merging (32, 110) into 272\n",
      "Merging (32, 111) into 273\n",
      "Merging (32, 263) into 274\n",
      "Merging (97, 114) into 275\n",
      "Merging (46, 10) into 276\n",
      "Merging (100, 111) into 277\n",
      "Merging (97, 110) into 278\n",
      "Merging (105, 110) into 279\n",
      "Merging (274, 101) into 280\n",
      "Merging (256, 256) into 281\n",
      "Merging (195, 163) into 282\n",
      "Merging (105, 115) into 283\n",
      "Merging (282, 111) into 284\n",
      "Merging (258, 101) into 285\n",
      "Merging (32, 118) into 286\n",
      "Merging (101, 109) into 287\n",
      "Merging (44, 10) into 288\n",
      "Merging (32, 102) into 289\n",
      "Merging (100, 97) into 290\n",
      "Merging (116, 101) into 291\n",
      "Merging (111, 109) into 292\n",
      "Merging (195, 169) into 293\n",
      "Merging (116, 111) into 294\n",
      "Merging (114, 97) into 295\n",
      "Merging (114, 101) into 296\n",
      "Merging (111, 110) into 297\n",
      "Merging (97, 108) into 298\n",
      "Merging (46, 46) into 299\n",
      "Merging (117, 109) into 300\n",
      "Merging (195, 161) into 301\n",
      "Merging (32, 108) into 302\n",
      "Merging (101, 105) into 303\n",
      "Merging (105, 109) into 304\n",
      "Merging (259, 115) into 305\n",
      "Merging (10, 10) into 306\n",
      "Merging (32, 104) into 307\n",
      "Merging (105, 97) into 308\n",
      "Merging (116, 97) into 309\n",
      "Merging (195, 167) into 310\n",
      "Merging (100, 101) into 311\n",
      "Merging (114, 111) into 312\n",
      "Merging (32, 98) into 313\n",
      "Merging (262, 101) into 314\n",
      "Merging (97, 109) into 315\n",
      "Merging (32, 293) into 316\n",
      "Merging (115, 111) into 317\n",
      "Merging (32, 300) into 318\n",
      "Merging (276, 10) into 319\n",
      "Merging (272, 284) into 320\n",
      "Merging (81, 117) into 321\n",
      "Merging (101, 108) into 322\n",
      "Merging (195, 179) into 323\n",
      "Merging (264, 292) into 324\n",
      "Merging (278, 277) into 325\n",
      "Merging (258, 111) into 326\n",
      "Merging (114, 105) into 327\n",
      "Merging (117, 115) into 328\n",
      "Merging (260, 101) into 329\n",
      "Merging (265, 100) into 330\n",
      "Merging (104, 97) into 331\n",
      "Merging (111, 117) into 332\n",
      "Merging (105, 290) into 333\n",
      "Merging (104, 101) into 334\n",
      "Merging (32, 10) into 335\n",
      "Merging (32, 279) into 336\n",
      "Merging (257, 108) into 337\n",
      "Merging (272, 111) into 338\n",
      "Merging (60, 330) into 339\n",
      "Merging (32, 80) into 340\n",
      "Merging (116, 105) into 341\n",
      "Merging (117, 110) into 342\n",
      "Merging (281, 281) into 343\n",
      "Merging (259, 109) into 344\n",
      "Merging (258, 97) into 345\n",
      "Merging (265, 291) into 346\n",
      "Merging (273, 117) into 347\n",
      "Merging (272, 97) into 348\n",
      "Merging (32, 103) into 349\n",
      "Merging (281, 256) into 350\n",
      "Merging (350, 32) into 351\n",
      "Merging (104, 111) into 352\n",
      "Merging (257, 115) into 353\n",
      "Merging (261, 110) into 354\n",
      "Merging (321, 101) into 355\n",
      "Merging (275, 97) into 356\n",
      "Merging (267, 111) into 357\n",
      "Merging (115, 101) into 358\n",
      "Merging (111, 108) into 359\n",
      "Merging (195, 173) into 360\n",
      "Merging (99, 97) into 361\n",
      "Merging (109, 97) into 362\n",
      "Merging (105, 118) into 363\n",
      "Merging (265, 115) into 364\n",
      "Merging (270, 268) into 365\n",
      "Merging (264, 297) into 366\n",
      "Merging (257, 110) into 367\n",
      "Merging (32, 266) into 368\n",
      "Merging (259, 117) into 369\n",
      "Merging (32, 119) into 370\n",
      "Merging (195, 170) into 371\n",
      "Merging (109, 101) into 372\n",
      "Merging (269, 317) into 373\n",
      "Merging (260, 261) into 374\n",
      "Merging (105, 108) into 375\n",
      "Merging (258, 269) into 376\n",
      "Merging (103, 117) into 377\n",
      "Merging (97, 283) into 378\n",
      "Merging (68, 101) into 379\n",
      "Merging (373, 97) into 380\n",
      "Merging (268, 97) into 381\n",
      "Merging (340, 380) into 382\n",
      "Merging (70, 354) into 383\n",
      "Merging (383, 325) into 384\n",
      "Merging (62, 384) into 385\n",
      "Merging (226, 128) into 386\n",
      "Merging (118, 101) into 387\n",
      "Merging (299, 276) into 388\n",
      "Merging (110, 99) into 389\n",
      "Merging (105, 114) into 390\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m toks \u001b[38;5;241m=\u001b[39m RegexTokenizer()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtoks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/FernandoPessoa/tokenizer/tok.py:77\u001b[0m, in \u001b[0;36mRegexTokenizer.train\u001b[0;34m(self, text, vocab_size, verbose)\u001b[0m\n\u001b[1;32m     74\u001b[0m stats \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk_ids \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[0;32m---> 77\u001b[0m     \u001b[43mget_pair_frequency\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m top_pair \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(stats, key\u001b[38;5;241m=\u001b[39mstats\u001b[38;5;241m.\u001b[39mget) \u001b[38;5;66;03m#returns most common pair\u001b[39;00m\n\u001b[1;32m     80\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m \u001b[38;5;241m+\u001b[39m i\n",
      "File \u001b[0;32m~/FernandoPessoa/tokenizer/tok.py:5\u001b[0m, in \u001b[0;36mget_pair_frequency\u001b[0;34m(tokens, counts)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_pair_frequency\u001b[39m(tokens, counts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m): \u001b[38;5;66;03m#checks for the number of token pairs in the text. Returns dictionary with pair: # of occurrences\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     counts\u001b[38;5;241m=\u001b[39m{}  \u001b[38;5;28;01mif\u001b[39;00m counts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m counts\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      6\u001b[0m         counts[pair] \u001b[38;5;241m=\u001b[39m counts\u001b[38;5;241m.\u001b[39mget(pair, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m counts\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "toks = RegexTokenizer()\n",
    "toks.train(poems, 3000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks_dict = {'vocab':toks.vocab, 'merges':toks.merges}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('toks_vocab_3k.pkl', 'wb') as f:\n",
    "    pickle.dump(toks_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vocab': {0: b'\\x00', 1: b'\\x01', 2: b'\\x02', 3: b'\\x03', 4: b'\\x04', 5: b'\\x05', 6: b'\\x06', 7: b'\\x07', 8: b'\\x08', 9: b'\\t', 10: b'\\n', 11: b'\\x0b', 12: b'\\x0c', 13: b'\\r', 14: b'\\x0e', 15: b'\\x0f', 16: b'\\x10', 17: b'\\x11', 18: b'\\x12', 19: b'\\x13', 20: b'\\x14', 21: b'\\x15', 22: b'\\x16', 23: b'\\x17', 24: b'\\x18', 25: b'\\x19', 26: b'\\x1a', 27: b'\\x1b', 28: b'\\x1c', 29: b'\\x1d', 30: b'\\x1e', 31: b'\\x1f', 32: b' ', 33: b'!', 34: b'\"', 35: b'#', 36: b'$', 37: b'%', 38: b'&', 39: b\"'\", 40: b'(', 41: b')', 42: b'*', 43: b'+', 44: b',', 45: b'-', 46: b'.', 47: b'/', 48: b'0', 49: b'1', 50: b'2', 51: b'3', 52: b'4', 53: b'5', 54: b'6', 55: b'7', 56: b'8', 57: b'9', 58: b':', 59: b';', 60: b'<', 61: b'=', 62: b'>', 63: b'?', 64: b'@', 65: b'A', 66: b'B', 67: b'C', 68: b'D', 69: b'E', 70: b'F', 71: b'G', 72: b'H', 73: b'I', 74: b'J', 75: b'K', 76: b'L', 77: b'M', 78: b'N', 79: b'O', 80: b'P', 81: b'Q', 82: b'R', 83: b'S', 84: b'T', 85: b'U', 86: b'V', 87: b'W', 88: b'X', 89: b'Y', 90: b'Z', 91: b'[', 92: b'\\\\', 93: b']', 94: b'^', 95: b'_', 96: b'`', 97: b'a', 98: b'b', 99: b'c', 100: b'd', 101: b'e', 102: b'f', 103: b'g', 104: b'h', 105: b'i', 106: b'j', 107: b'k', 108: b'l', 109: b'm', 110: b'n', 111: b'o', 112: b'p', 113: b'q', 114: b'r', 115: b's', 116: b't', 117: b'u', 118: b'v', 119: b'w', 120: b'x', 121: b'y', 122: b'z', 123: b'{', 124: b'|', 125: b'}', 126: b'~', 127: b'\\x7f', 128: b'\\x80', 129: b'\\x81', 130: b'\\x82', 131: b'\\x83', 132: b'\\x84', 133: b'\\x85', 134: b'\\x86', 135: b'\\x87', 136: b'\\x88', 137: b'\\x89', 138: b'\\x8a', 139: b'\\x8b', 140: b'\\x8c', 141: b'\\x8d', 142: b'\\x8e', 143: b'\\x8f', 144: b'\\x90', 145: b'\\x91', 146: b'\\x92', 147: b'\\x93', 148: b'\\x94', 149: b'\\x95', 150: b'\\x96', 151: b'\\x97', 152: b'\\x98', 153: b'\\x99', 154: b'\\x9a', 155: b'\\x9b', 156: b'\\x9c', 157: b'\\x9d', 158: b'\\x9e', 159: b'\\x9f', 160: b'\\xa0', 161: b'\\xa1', 162: b'\\xa2', 163: b'\\xa3', 164: b'\\xa4', 165: b'\\xa5', 166: b'\\xa6', 167: b'\\xa7', 168: b'\\xa8', 169: b'\\xa9', 170: b'\\xaa', 171: b'\\xab', 172: b'\\xac', 173: b'\\xad', 174: b'\\xae', 175: b'\\xaf', 176: b'\\xb0', 177: b'\\xb1', 178: b'\\xb2', 179: b'\\xb3', 180: b'\\xb4', 181: b'\\xb5', 182: b'\\xb6', 183: b'\\xb7', 184: b'\\xb8', 185: b'\\xb9', 186: b'\\xba', 187: b'\\xbb', 188: b'\\xbc', 189: b'\\xbd', 190: b'\\xbe', 191: b'\\xbf', 192: b'\\xc0', 193: b'\\xc1', 194: b'\\xc2', 195: b'\\xc3', 196: b'\\xc4', 197: b'\\xc5', 198: b'\\xc6', 199: b'\\xc7', 200: b'\\xc8', 201: b'\\xc9', 202: b'\\xca', 203: b'\\xcb', 204: b'\\xcc', 205: b'\\xcd', 206: b'\\xce', 207: b'\\xcf', 208: b'\\xd0', 209: b'\\xd1', 210: b'\\xd2', 211: b'\\xd3', 212: b'\\xd4', 213: b'\\xd5', 214: b'\\xd6', 215: b'\\xd7', 216: b'\\xd8', 217: b'\\xd9', 218: b'\\xda', 219: b'\\xdb', 220: b'\\xdc', 221: b'\\xdd', 222: b'\\xde', 223: b'\\xdf', 224: b'\\xe0', 225: b'\\xe1', 226: b'\\xe2', 227: b'\\xe3', 228: b'\\xe4', 229: b'\\xe5', 230: b'\\xe6', 231: b'\\xe7', 232: b'\\xe8', 233: b'\\xe9', 234: b'\\xea', 235: b'\\xeb', 236: b'\\xec', 237: b'\\xed', 238: b'\\xee', 239: b'\\xef', 240: b'\\xf0', 241: b'\\xf1', 242: b'\\xf2', 243: b'\\xf3', 244: b'\\xf4', 245: b'\\xf5', 246: b'\\xf6', 247: b'\\xf7', 248: b'\\xf8', 249: b'\\xf9', 250: b'\\xfa', 251: b'\\xfb', 252: b'\\xfc', 253: b'\\xfd', 254: b'\\xfe', 255: b'\\xff', 256: b'  ', 257: b' a', 258: b' d'}, 'merges': {(32, 32): 256, (32, 97): 257, (32, 100): 258}}\n"
     ]
    }
   ],
   "source": [
    "with open('toks_vocab_3k.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "print(vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
