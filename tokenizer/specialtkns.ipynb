{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ola ', '<end>', ' ola ', '<end>', '']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_pattern = \"(\"+ \"|\".join(token for token in special.keys()) +\")\"\n",
    "specialchunks = re.split(special_pattern , s)\n",
    "specialchunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_frequency(tokens, counts = None): #checks for the number of token pairs in the text. Returns dictionary with pair: # of occurrences\n",
    "    counts={}  if counts is None else counts\n",
    "    for pair in zip(tokens, tokens[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge_pair(ids, pair, idx):\n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i< len(ids)-1 and pair[0] == ids[i] and pair[1] == ids[i+1]:\n",
    "            new_ids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_ids.append(ids[i])\n",
    "            i += 1\n",
    "    return new_ids\n",
    "\n",
    "class Tokenizer():\n",
    "    def __init__(self):\n",
    "        self.vocab = {idx: bytes([idx]) for idx in range(256)}        \n",
    "        self.merges = {}\n",
    "    \n",
    "    def train(self, vocab_size, text):\n",
    "        tokens = text.encode('utf-8')\n",
    "        for i in range(vocab_size - 256):\n",
    "            pair_frequency = get_pair_frequency(tokens)\n",
    "            top_pair = max(pair_frequency, key=pair_frequency.get) #returns pair that appear the most\n",
    "            idx = len(self.vocab)\n",
    "            print(f'Merging {top_pair} -> {idx}')\n",
    "            self.merges[top_pair] = idx\n",
    "            tokens = merge_pair(tokens, top_pair, idx)\n",
    "            self.vocab[idx] = self.vocab[top_pair[0]] + self.vocab[top_pair[1]]\n",
    "    \n",
    "    def encode(self, text):\n",
    "        tokens = list(text.encode('utf-8'))\n",
    "        \n",
    "        while len(tokens) > 1:\n",
    "            pair_frequency = get_pair_frequency(tokens)\n",
    "            pair = min(pair_frequency, key = lambda p:self.merges.get(p, float(\"inf\"))) #returns pair that was merged first\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "        \n",
    "            idx = self.merges[pair] #get encoded id\n",
    "            tokens = merge_pair(tokens, pair, idx)\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        tokens = b\"\".join([self.vocab[idx] for idx in ids])\n",
    "        text = tokens.decode(\"utf-8\", errors='replace') #translates bytes to characters\n",
    "        return text\n",
    "\n",
    "\n",
    "class RegexTokenizer(Tokenizer):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.pattern = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"  #GPT split pattern\n",
    "        self.compiled_pattern = re.compile(self.pattern)\n",
    "        self.merges = {} # (int, int) -> int\n",
    "        self.vocab = {idx: bytes([idx]) for idx in range(256)} # idx -> bytes\n",
    "    \n",
    "\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        self.special = {'<end>': vocab_size} #assign a integer value to the special token to encode them.\n",
    "\n",
    "        num_merges = vocab_size - 256\n",
    "\n",
    "        text_chunks = re.findall(self.compiled_pattern, text)        \n",
    "        ids = [list(ch.encode(\"utf-8\")) for ch in text_chunks]\n",
    "        \n",
    "        for i in range(num_merges):\n",
    "\n",
    "            stats = {}\n",
    "\n",
    "            for chunk_ids in ids:\n",
    "                get_pair_frequency(chunk_ids, stats)\n",
    "\n",
    "            top_pair = max(stats, key=stats.get) #returns most common pair\n",
    "            idx = 256 + i\n",
    "            \n",
    "            if verbose:\n",
    "                print(f'Merging {top_pair} into {idx}')\n",
    "            \n",
    "            ids = [merge_pair(chunk_ids, top_pair, idx) for chunk_ids in ids]\n",
    "            \n",
    "            self.merges[top_pair] = idx\n",
    "            self.vocab[idx] = self.vocab[top_pair[0]] + self.vocab[top_pair[1]]\n",
    "\n",
    "    def _encode_chunk(self, chunk_bytes):\n",
    "        ids = list(chunk_bytes)\n",
    "        while len(ids)>=2:\n",
    "            stats = get_pair_frequency(ids)\n",
    "            pair = min(stats, key=lambda p:self.merges.get(p, float(\"inf\"))) #get the pair that has was merged first\n",
    "            if pair not in self.merges:\n",
    "                break #nothing else to merge\n",
    "            \n",
    "            idx = self.merges[pair]\n",
    "            ids = merge_pair(ids, pair, idx)\n",
    "        \n",
    "        return ids\n",
    "\n",
    "    def encode(self, text):\n",
    "        #start by encoding special characters\n",
    "        special_pattern = \"(\"+ \"|\".join(token for token in self.special.keys()) +\")\" #wraps special tokens around \"()\" to make it a capturing group and be included in the split\n",
    "        special_chunks = re.split(special_pattern , text)\n",
    "        ids = []\n",
    "\n",
    "        for part in special_chunks:\n",
    "            if part in self.special.keys():\n",
    "                ids.append(self.special[part])\n",
    "\n",
    "            else:\n",
    "                # chunks encoded separately and then merged together\n",
    "                text_chunks = re.findall(self.compiled_pattern, part)\n",
    "\n",
    "                for chunk in text_chunks:\n",
    "                    chunk_bytes = chunk.encode(\"utf-8\")\n",
    "                    chunk_ids = self._encode_chunk(chunk_bytes)\n",
    "                    ids.extend(chunk_ids)\n",
    "        \n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        partial_tokens = []\n",
    "        inverse_special = {v:k for k,v in self.special.items()}\n",
    "        for idx in ids:\n",
    "            if idx in self.vocab:\n",
    "                partial_tokens.append(self.vocab[idx])\n",
    "            elif idx in inverse_special.keys():\n",
    "                partial_tokens.append(inverse_special[idx].encode(\"utf-8\"))\n",
    "            else:\n",
    "                raise ValueError(f\"token {idx} not in vocab\")\n",
    "        tokens = b\"\".join(partial_tokens) #ids(list of integers as encoded) -> bytes\n",
    "        text = tokens.decode(\"utf-8\", errors='replace') #bytes -> characters\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging (111, 108) into 256\n",
      "Merging (256, 97) into 257\n",
      "Merging (32, 60) into 258\n",
      "Merging (101, 110) into 259\n",
      "Merging (259, 100) into 260\n",
      "Merging (32, 97) into 261\n",
      "Merging (261, 98) into 262\n",
      "Merging (32, 257) into 263\n",
      "Merging (262, 99) into 264\n"
     ]
    }
   ],
   "source": [
    "s = \"ola <end> ola <end> abc ab\"\n",
    "tok = RegexTokenizer()\n",
    "tok.train(s, 265,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[257, 32, 265, 263, 32, 265, 264, 262]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s)\n",
    "tok.encode(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ola <end> ola <end> abc ab'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.decode(tok.encode(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
